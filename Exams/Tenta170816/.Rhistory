prior_dens
prior_dens = dnorm(theta_grid, 0, 10)
prior_dens = dnorm(1, 0, 10)
prior_dens
prior_dens = dnorm(2, 0, 10)
prior_dens
prior_dens = dnorm(theta_grid, 0, 10)
prior_dens
for (theta in theta_grid){
likelihood = dCauchy(yVect, theta, 1)
}
posterior = C()
for (theta in theta_grid){
likelihood = dCauchy(yVect, theta, 1)
prior = dnorm(theta, 0, 10)
post = likelihood*prior
posterior = append(posterior, post)
}
posterior = C()
for (theta in theta_grid){
likelihood = dCauchy(yVect, theta, 1)
prior = dnorm(theta, 0, 10)
post = likelihood*prior
posterior = append(posterior, post)
}
posterior = c()
for (theta in theta_grid){
likelihood = dCauchy(yVect, theta, 1)
prior = dnorm(theta, 0, 10)
post = likelihood*prior
posterior = append(posterior, post)
}
posterior
plot(density(posterior))
post_norm = posterior/sum(posterior)
post_norm
hist(post_norm)
plot(density(posterior))
dCauchy <- function(x, theta = 0, gamma = 1){
return(dens = (1/(pi*gamma))*(1/(1+((x-theta)/gamma)^2)))
}
theta_grid = seq(0,10, 0.1)
posterior = c()
for (theta in theta_grid){
likelihood = likelihood*dCauchy(yVect, theta, 1)
prior = dnorm(theta, 0, 10)
post = likelihood*prior
posterior = append(posterior, post)
}
post_norm = posterior/sum(posterior)
hist(post_norm)
hist(post_norm, 40)
hist(post_norm, 400)
posterior = c()
for (theta in theta_grid){
likelihood = dCauchy(yVect, theta, 1)
prior = dnorm(theta, 0, 10)
post = likelihood*prior
posterior = append(posterior, post)
}
post_norm = posterior/sum(posterior)
hist(post_norm)
likelihood
log.like = log(dCauchy(yVect, 1, 1))
log.like
log.like = sum(log(dCauchy(yVect, 1, 1)))
log.like
log.likelihood = function(yVect, theta){
log.like = sum(log(dCauchy(yVect, theta, 1)))
}
posterior = c()
for (theta in theta_grid){
log.likelihood = log.likelihood(yVect, theta)
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood*prior
posterior = append(exp(posterior), post)
}
posterior = c()
for (theta in theta_grid){
log.likelihood = log.likelihood(yVect, theta)
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood*prior
posterior = append(posterior, exp(log.post))
}
log.likelihood = function(yVect, theta){
log.like = sum(log(dCauchy(yVect, theta, 1)))
}
posterior = c()
for (theta in theta_grid){
log.likelihood = log.likelihood(yVect, theta)
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood*prior
posterior = append(posterior, exp(log.post))
}
post_norm = posterior/sum(posterior)
log.likelihood = function(yVect, theta){
log.like = sum(log(dCauchy(yVect, theta, 1)))
}
posterior = c()
for (theta in theta_grid){
log.likelihood = log.likelihood(yVect, theta)
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood*prior
posterior = append(posterior, exp(log.post))
}
log.likelihood = log.likelihood(yVect, theta)
log.likelihood = function(yVect, theta){
log.like = sum(log(dCauchy(yVect, theta, 1)))
}
log.like = function(yVect, theta){
likelihood= sum(log(dCauchy(yVect, theta, 1)))
}
for (theta in theta_grid){
log.likelihood = log.like(yVect, theta)
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood*prior
posterior = append(posterior, exp(log.post))
}
plot(posterior)
plot(density(posterior))
posterior = c()
for (theta in theta_grid){
log.likelihood = log.like(yVect, theta)
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + prior
posterior = append(posterior, exp(log.post))
}
plot(density(posterior))
posterior = c()
for (theta in theta_grid){
log.likelihood = log.like(yVect, theta)
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
posterior = append(posterior, exp(log.post))
}
plot(density(posterior))
post_norm = posterior/sum(posterior)
hist(post_norm)
log.posterior = function(yVect, theta){
likelihood= sum(log(dCauchy(yVect, theta, 1)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log.post))
}
plot(density(posterior))
theta_grid
gridWidth = theta_grid[1] - theta_grid[2]
gridWidth = theta_grid[1] - theta_grid[2]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(theta_grid, post_norm)
length(posterior)
length(theta_gt )
length(theta_grid)
#
# Reading the data vector yVect from file
load(file = 'CauchyData.RData')
dCauchy <- function(x, theta = 0, gamma = 1){
return(dens = (1/(pi*gamma))*(1/(1+((x-theta)/gamma)^2)))
}
theta_grid = seq(0,10, 0.1)
log.posterior = function(yVect, theta){
likelihood= sum(log(dCauchy(yVect, theta, 1)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log.post))
}
gridWidth = theta_grid[1] - theta_grid[2]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(theta_grid, post_norm)
hist(post_norm)
length(posterior)
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log_posterior))
}
posterior
length(posterior)
length(theta_grid)
plot(theta_grid, post_norm)
gridWidth = theta_grid[1] - theta_grid[2]
post_norm = (1/gridWidth)*posterior/sum(posterior)
post_norm
plot()
plot(post_norm)
#
# Reading the data vector yVect from file
load(file = 'CauchyData.RData')
dCauchy <- function(x, theta = 0, gamma = 1){
return(dens = (1/(pi*gamma))*(1/(1+((x-theta)/gamma)^2)))
}
theta_grid = seq(0,10, 0.1)
log.posterior = function(yVect, theta){
likelihood= sum(log(dCauchy(yVect, theta, 1)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log_posterior))
}
plot(posterior)
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(post_norm)
plot(density(posterior))
gridWidth
gridWidth = theta_grid[2] - theta_grid[1]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(post_norm)
plot(theta_grid, post_norm)
theta_grid = seq(3,8, length = 1000)
theta_grid = seq(3,8, length = 1000)
log.posterior = function(yVect, theta){
likelihood= sum(log(dCauchy(yVect, theta, 1)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log_posterior))
}
plot(posterior)
plot(density(posterior))
gridWidth = theta_grid[2] - theta_grid[1]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(theta_grid, post_norm)
logPostCauchyUnitScale <- function(theta, yVect){
sum(log(dCauchy(yVect, theta, gamma = 1))) + dnorm(theta, mean = 0, sd = 10, log = TRUE)
}
thetaGrid <- seq(0, 12, length = 1000)
logPostEvals <- rep(0, 1000)
i = 0
for (theta in thetaGrid){
i = i + 1
logPostEvals[i] <- logPostCauchyUnitScale(theta, yVect)
}
binWidth = thetaGrid[2]-thetaGrid[1]
plot(thetaGrid, exp(logPostEvals)/(sum(exp(logPostEvals))*binWidth), type = "l", ylab = 'Posterior density', xlab = expression(theta))
p = dlognormal(1, 0, 10)
p
log.posterior = function(yVect, theta){
sigma_draw = dlognormal(theta, 0, 10)
likelihood= sum(log(dCauchy(yVect, theta, sigma_draw)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
plot(density(posterior))
log.posterior = function(yVect, theta){
sigma_draw = dlognormal(theta, 0, 10)
likelihood= sum(log(dCauchy(yVect, theta, sigma_draw)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log_posterior))
}
gridWidth = theta_grid[2] - theta_grid[1]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(theta_grid, post_norm)
log.posterior = function(yVect, theta){
#sigma_draw = dlognormal(theta, 0, 10)
likelihood= sum(log(dCauchy(yVect, theta, 1)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log_posterior))
}
gridWidth = theta_grid[2] - theta_grid[1]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(theta_grid, post_norm)
log.posterior = function(yVect, theta){
sigma_draw = dlognormal(theta, 0, 10)
likelihood= sum(log(dCauchy(yVect, theta, sigma_draw)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log_posterior))
}
gridWidth = theta_grid[2] - theta_grid[1]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(theta_grid, post_norm)
log.posterior = function(yVect, theta){
sigma_draw = dlognormal(theta, 0, 1)
likelihood= sum(log(dCauchy(yVect, theta, sigma_draw)))
log.prior = dnorm(theta, 0, 10, log = TRUE)
log.post = likelihood + log.prior
return(log.post)
}
posterior = c()
for (theta in theta_grid){
log_posterior = log.posterior(yVect, theta)
posterior = append(posterior, exp(log_posterior))
}
gridWidth = theta_grid[2] - theta_grid[1]
post_norm = (1/gridWidth)*posterior/sum(posterior)
plot(theta_grid, post_norm)
theta = params[1]
sigma = params[2]
log.posterior = function(params, yVect){
theta = params[1]
sigma = params[2]
log.likelihood= sum(log(dCauchy(yVect, theta, sigma)))
log.theta.prior = dnorm(theta, 0, 10, log = TRUE)
log.sigma.prior = log(dlognormal(sigma, 0, 1))
log.post = log.likelihood + log.theta.prior + log.sigma.prior
return(log.post)
}
# Different starting values. Ideally, any random starting value gives you the same optimum (i.e. optimum is unique)
initVal <- as.vector(rep(0,2));
initVal
# function which optmizes over expression log.posterior with respect to its first argument (betas).
# returns optimal values for beta (mode), and hessian in the mode
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"), control=list(fnscale=-1),hessian=TRUE)
# function which optmizes over expression log.posterior with respect to its first argument (betas).
# returns optimal values for beta (mode), and hessian in the mode
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# function which optmizes over expression log.posterior with respect to its first argument (betas).
# returns optimal values for beta (mode), and hessian in the mode
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("BFGS"), lower = c(0,0), control=list(fnscale=-1),hessian=TRUE)
# function which optmizes over expression log.posterior with respect to its first argument (betas).
# returns optimal values for beta (mode), and hessian in the mode
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"), lower = c(0,0), control=list(fnscale=-1),hessian=TRUE)
#Är det inte märkligt att köra X en dragning fråån log normal ? som i lab1 vad händer ?
log.posterior = function(params, yVect){
theta = params[1]
sigma = params[2]
log.likelihood= sum(log(dCauchy(yVect, theta, sigma)))
log.theta.prior = dnorm(theta, 0, 10, log = TRUE)
log.sigma.prior = log(dlognormal(sigma, 0, 1))
log.post = log.likelihood + log.theta.prior + log.sigma.prior
return(log.post)
}
# Different starting values. Ideally, any random starting value gives you the same optimum (i.e. optimum is unique)
initVal <- as.vector(rep(0,2));
# function which optmizes over expression log.posterior with respect to its first argument (betas).
# returns optimal values for beta (mode), and hessian in the mode
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"), lower = c(0,0), control=list(fnscale=-1),hessian=TRUE)
# function which optmizes over expression log.posterior with respect to its first argument (betas).
# returns optimal values for beta (mode), and hessian in the mode
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"),lower = c(-Inf,0.0001), upper = c(Inf,Inf), control=list(fnscale=-1),hessian=TRUE)
betas_posterior = OptParams$par
# takes negative so that the posterior can be approx. as normal
# J = -second derivate evaluated in theta_hat
hessian_posterior = -OptParams$hessian
# take inverse for using it in the formula
hessian_posterior = solve(hessian_posterior)
betas_posterior
## use the normal approximation above to obtain the marginal posterior for the 99% percentile of teh candy distribution
# Theta + gamma*tan(pi(0.99 - 0.5))
library(mvtnorm)
betas_posterior
pi
theta = betas_posterior[1]
theta
gamma = betas_posterior[2]
gamma = betas_posterior[2]*tan(pi(0.99-0.5))
(pi(0.99-0.5))
pi
gamma = betas_posterior[2]*tan(pi*(0.99-0.5))
gamma
gamma = betas_posterior[2]*tan(pi*(0.99-0.5))
gamma
m = theta + gamma
p = rmvnorm(1, mean = m, sigma =hessian_posterior )
# take inverse for using it in the formula
hessian_posterior = solve(hessian_posterior)
p = rmvnorm(1, mean = m, sigma =hessian_posterior )
## use the normal approximation above to obtain the marginal posterior for the 99% percentile of teh candy distribution
# Theta + gamma*tan(pi(0.99 - 0.5))
library(mvtnorm)
theta = betas_posterior[1]
gamma = betas_posterior[2]*tan(pi*(0.99-0.5))
m = theta + gamma
p = rmvnorm(1, mean = m, sigma =hessian_posterior )
m
hessian_posterior
p = rmvnorm(1, mean = betas_posterior, sigma =hessian_posterior )
p
p = rmvnorm(1000, mean = betas_posterior, sigma =hessian_posterior )
p
cauchy = draws[,1] + draws[,2]*tan(pi*(0.9 - 0.5))
draws = rmvnorm(1000, mean = betas_posterior, sigma =hessian_posterior )
cauchy = draws[,1] + draws[,2]*tan(pi*(0.9 - 0.5))
cauchy
summary(caucy)
summary(cauchy)
quantile(draws, pros = seq(0,1 ))
quantile(draws, pros = seq(0,1, 0.01 ))
quantile(draws, pros = seq(0, 0.01 ))
quantile(draws, probs = seq(0, 0.01 ))
quantile(draws, probs = seq(0, 0.99))
quantile(draws, probs = seq(0,1, 0.01))
cauchy = draws[,1] + draws[,2]*tan(pi*(0.9 - 0.5))
cauchy
cauchy = draws[,1] + draws[,2]*tan(pi*(0.99 - 0.5))
quantile(draws, probs = seq(0,1, 0.01))
cauchy
# Reading the data from file
library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
Boston
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
names(X)[1] <- "intercept"
covNames <- names(X)
y <- as.numeric(y)
X <- as.matrix(X)
Y
y
X
install.packages("mvtnorm")
install.packages("mvtnorm")
library(mvtnorm)
# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
return((df*scale)/rchisq(n,df=df))
}
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
boston
BostonHousing
var(X)
sd(X)
my_0 = c(0,0)
Omega_0 = diag(1, 2)
Omega_0 = Omega_0*0.01
Omega_0
sigma = sd(X)
sigma
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = 4, sigma2_0 = sigma^2 , nIter = 100 )
X
Omega_0 = diag(1, 14)
Omega_0 = Omega_0*0.01
sigma = sd(X)
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = 4, sigma2_0 = sigma^2 , nIter = 100 )
my_0 = rep(0, 14)
Omega_0 = diag(1, 14)
Omega_0 = Omega_0*0.01
sigma = sd(X)
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = 4, sigma2_0 = sigma^2 , nIter = 100 )
joint_posterior
