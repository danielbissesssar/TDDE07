sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
###############################
########## Problem 3 ##########
###############################
# No code or data for this problem
###############################
########## Problem 4 ##########
###############################
# No code or data for this problem
yprop
yProp
thetaGrid = seq(0.01,15, length =1000)
k_prior = function(lambda, k){
lambda*exp(-lambda*k)
}
k_prior_vector = k_prior(lambda, thetaGrid)
thetaGrid = seq(0.01,15, length =1000)
lambda = 1
k_prior = function(lambda, k){
lambda*exp(-lambda*k)
}
k_prior_vector = k_prior(lambda, thetaGrid)
k_prior_vector
posterior.calc = function(y, theta){
log.post = sum((theta-1)*log(y)+(theta-1)*log(1-y)) + lambda*exp(-lambda*theta)
}
posterior.calc = function(y, theta){
log.post = sum((theta-1)*log(y)+(theta-1)*log(1-y)) + lambda*exp(-lambda*theta)
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
posterior
plot(density(posterior))
log.post = sum((2-1)*log(y)+(2-1)*log(1-y)) + lambda*exp(-lambda*2)
temp = sum((3-1)*log(yProp)+(3-1)*log(1-yProp)) + lambda*exp(-lambda*3)
temp = sum((3-1)*log(yProp)+(3-1)*log(1-yProp)) + lambda*exp(-lambda*3)
temp
exp(temp)
length(yProp)
posterior.calc = function(y, theta){
likelihood = 0
for(i in length(y)){
likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
}
log.post = likelihood + lambda*exp(-lambda*theta)
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
plot(density(posterior))
plot(density(thetaGrid, posterior))
gridwidth = thetaGrid[2] - thetaGrid[1]
grid()
gridwidth
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(density(thetaGrid, posterior_nromalized))
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = posterior/sum(posterior)
plot(density(thetaGrid, posterior_nromalized))
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(density(thetaGrid, posterior_nromalized))
posterior
posterior_nromalized
point_estimate = median(posterior)
point_estimate
logPostSymmetricBeta <- function(theta, yProp){
sum(log(dbeta(yProp, shape1 = theta, shape2 = theta))) + dexp(theta, rate = 1, log = TRUE)
}
thetaGrid = seq(0.01, 15, length = 1000)
logPostEvals <- rep(0, 1000)
i = 0
for (theta in thetaGrid){
i = i + 1
logPostEvals[i] <- logPostSymmetricBeta(theta, yProp)
}
binWidth = thetaGrid[2]-thetaGrid[1]
plot(thetaGrid, exp(logPostEvals)/(sum(exp(logPostEvals))*binWidth), type = "l", ylab = 'Posterior density', xlab = expression(theta))
# It is ok also to plot the unnormalized density.
thetaGrid = seq(0.01,15, length =1000)
lambda = 1
posterior.calc = function(y, theta){
likelihood = 0
for(i in length(y)){
likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
}
log.post = likelihood + log(lambda*exp(-lambda*theta))
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(density(thetaGrid, posterior_nromalized))
likelihood = dgamma(1, theta, theat)
dgamma(1, 1, 1)
dgamma(yProp, 5, 5)
likelihood = dgamma(y, theta, theta, log = TRUE)
dgamma(yProp, 5, 5, log = TRUE)
thetaGrid = seq(0.01,15, length =1000)
lambda = 1
posterior.calc = function(y, theta){
#  for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
# }
# log.post = likelihood + log(lambda*exp(-lambda*theta))
likelihood = sum(dgamma(y, theta, theta, log = TRUE))
log.prior = dexp(theta, 1)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(density(thetaGrid, posterior_nromalized))
posterior.calc = function(y, theta){
#  for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
# }
# log.post = likelihood + log(lambda*exp(-lambda*theta))
likelihood = sum(dgamma(y, theta, theta, log = TRUE))
log.prior = dexp(theta, 1)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
posterior
posterior.calc = function(y, theta){
#  for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
# }
# log.post = likelihood + log(lambda*exp(-lambda*theta))
likelihood = sum(dgamma(y, theta, theta, log = TRUE))
log.prior = dexp(theta, 1, log = TRUE)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(density(thetaGrid, posterior_nromalized))
thetaGrid = seq(0.01,15, length =1000)
lambda = 1
posterior.calc = function(y, theta){
#  for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
# }
# log.post = likelihood + log(lambda*exp(-lambda*theta))
likelihood = sum(log(dgamma(y, theta, theta)))
log.prior = dexp(theta, 1, log = TRUE)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(density(thetaGrid, posterior_nromalized))
plot(density(posterior_nromalized))
plot(thetaGrid, posterior_nromalized)
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
plot(density(posterior))
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(density(posterior_nromalized))
plot(thetaGrid, posterior_nromalized)
likelihood = sum((theta-1)*log(yProp)+(theta-1)*log(1-yProp))
likelihood
theta
likelihood = sum((3-1)*log(yProp)+(3-1)*log(1-yProp))
likelihood = sum((3-1)*log(yProp)+(3-1)*log(1-yProp)) + dexp(3, rate = 1, log = TRUE)
likelihood
exp(likelihood)
likelihood = ((3-1)*log(yProp)+(3-1)*log(1-yProp)) + dexp(3, rate = 1, log = TRUE)
likelihood
yProp
posterior.calc = function(y, theta){
likelihood = sum((theta-1)*log(yProp)+(theta-1)*log(1-yProp))
#for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
#}
# log.post = likelihood + log(lambda*exp(-lambda*theta))
#likelihood = sum(log(dgamma(y, theta, theta)))
log.prior = dexp(theta, rate = 1, log = TRUE)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
posterior
posterior = c()
posterior
posterior.calc = function(y, theta){
likelihood = sum((theta-1)*log(yProp)+(theta-1)*log(1-yProp))
#for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
#}
# log.post = likelihood + log(lambda*exp(-lambda*theta))
#likelihood = sum(log(dgamma(y, theta, theta)))
log.prior = dexp(theta, rate = 1, log = TRUE)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(thetaGrid, posterior_nromalized)
posterior.calc = function(y, theta){
#  for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
# }
# log.post = likelihood + log(lambda*exp(-lambda*theta))
likelihood = sum(log(dgamma(y, theta, theta)))
log.prior = dexp(theta, rate = 1, log = TRUE)
log.post = likelihood + log.prior
return(exp(log.post))
}
likelihood
likelihood1 = sum(log(dgamma(yProp, 3, 3)))
likelihood2 = sum((3-1)*log(yProp)+(3-1)*log(1-yProp))
likelihood1
likelihood2
likelihood2 = 0
likelihood2
for(i in length(yProp)){
likelihood2 = likelihood2 + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
}
likelihood2 = prod(yProp^(3-1)*(1-yProp)^(3-1))
likelihood2
log(likelihood2)
likelihood1
yProp
thetaGrid = seq(0.01,15, length =1000)
lambda = 1
posterior.calc = function(y, theta){
#  for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
# }
# log.post = likelihood + log(lambda*exp(-lambda*theta))
likelihood = sum(log(dgamma(y, theta, theta)))
log.prior = dexp(theta, rate = 1, log = TRUE)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(thetaGrid, posterior_nromalized)
#zero 1 loss => posterior mode
posterior_mode = thetaGrid[which.max(posterior_nromalized)]
posterior_mode
log.posterior = function(params, yVect){
theta1 = params[1]
theta2 = params[2]
likelihood = sum(log(dgamma(y, theta, theta)))
log.prior1 = dexp(theta1, rate = 1, log = TRUE)
log.prior2 = dexp(thet2, rate = 1, log = TRUE)
log.post = likelihood + log.prior1 + log.prior2
return(log.post)
}
initVal <- as.vector(rep(0,2));
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"),lower = c(0.0001,0.0001), control=list(fnscale=-1),hessian=TRUE)
## b
log.posterior = function(params, yVect){
theta1 = params[1]
theta2 = params[2]
likelihood = sum(log(dgamma(y, theta1, theta2)))
log.prior1 = dexp(theta1, rate = 1, log = TRUE)
log.prior2 = dexp(thet2, rate = 1, log = TRUE)
log.post = likelihood + log.prior1 + log.prior2
return(log.post)
}
initVal <- as.vector(rep(0,2));
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"),lower = c(0.0001,0.0001), control=list(fnscale=-1),hessian=TRUE)
## b
log.posterior = function(params, yVect){
theta1 = params[1]
theta2 = params[2]
likelihood = sum(log(dgamma(y, theta1, theta2)))
log.prior1 = dexp(theta1, rate = 1, log = TRUE)
log.prior2 = dexp(theta2, rate = 1, log = TRUE)
log.post = likelihood + log.prior1 + log.prior2
return(log.post)
}
initVal <- as.vector(rep(0,2));
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"),lower = c(0.0001,0.0001), control=list(fnscale=-1),hessian=TRUE)
my_posterior = OptParams$par
hessian_posterior = -OptParams$hessian
my_posterior
hessian_posterior = -OptParams$hessian
hessian_posterior
# Bayesian Learning Exam 2017-10-27
# Run this file once during the exam to get all the required data and functions for the exam in working memory
# Author: Mattias Villani
###############################
########## Problem 1 ##########
###############################
# Reading the data vector yProp from file
load(file = 'yProportions.RData')
###############################
########## Problem 2 ##########
###############################
# Reading the data from file
#library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
names(X)[1] <- "intercept"
covNames <- names(X)
y <- as.numeric(y)
X <- as.matrix(X)
#install.packages("mvtnorm")
#library(mvtnorm)
# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
return((df*scale)/rchisq(n,df=df))
}
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
###############################
########## Problem 3 ##########
###############################
# No code or data for this problem
###############################
########## Problem 4 ##########
###############################
# No code or data for this problem
X
Y
y
#2a)
y
X
ncol(X)
#2a)
my_0 = rep(0, ncol(X))
Omega_0 = diag(1, ncol(X))
Omega_0 = Omega_0*0.01
sigma = 6^2
v_0 = 1
Omega_0
#2a)
mu_0 = rep(0, ncol(X))
mu_0
nSim = 5000
joint_posterior <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter = nSim)
joint_posterior <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0 = sigma, nIter = nSim)
sigma
joint_posterior
X
interval = quantile(joint_posterior$betaSample[,14], probs = seq(0,1,0.05))
interval
joint_posterior$betaSample[,14]
sort(joint_posterior$betaSample[,14])
sort(joint_posterior$betaSample[,14], descending = TRUE)
sort(joint_posterior$betaSample[,14], ascending = FALSE)
sort(joint_posterior$betaSample[,14])
library(HDInterval)
install.packages("HDInterval")
library(HDInterval)
#ibrary(HDInterval)
hdiRange = hdi(joint_posterior$betaSample[,14], credMass=0.9)
hdiRange
#ibrary(HDInterval)
hdiRange = hdi(joint_posterior$betaSample[,14], credMass=0.95)
hdiRange
plot(joint_posterior$betaSample[,14])
#ibrary(HDInterval)
hdiRange = hdi(joint_posterior$betaSample[,14], credMass=0.95)
hdiRange
quantile(joint_posterior, probs = seq(0,1, 0.025))
quantile(joint_posterior, probs = seq(0,1, 0.025))
joint_posterior <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0 = sigma, nIter = nSim)
quantile(joint_posterior, probs = seq(0,1, 0.025))
quantile(joint_posterior[,14], probs = seq(0,1, 0.025))
quantile(joint_posterior[,14], probs = seq(0,1, 0.025))
quantile(joint_posterior$betaSample[,14], probs = seq(0,1, 0.025))
hdiRange
prediction_data_original = X[9,]
prediction_data_original = X[9,]
prediction_data_new = X[9,]
prediction_data_new[14] =  prediction_data_new[14]*0.7
prediciton_data_original
prediction_data_original
prediction_data_new
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
mdv_pred_original = c()
set.seed(12345)
mdv_pred_original = c()
for (i in 1:nrow(betas)){
pred = sum(betas[i,]*prediction_data_original) + rnorm(1, 0, sqrt(sigmas[i]))
mdv_pred_original = append(mdv_pred, pred)
}
for (i in 1:nrow(betas)){
pred = sum(betas[i,]*prediction_data_original) + rnorm(1, 0, sqrt(sigmas[i]))
mdv_pred_original = append(mdv_pred_original, pred)
}
mdv_pred_original
histhist(mdv_pred_original)
hist(mdv_pred_original)
mdv_pred_original = c()
mdv_pred_new = c()
for (i in 1:nrow(betas)){
pred = sum(betas[i,]*prediction_data_original) + rnorm(1, 0, sqrt(sigmas[i]))
mdv_pred_original = append(mdv_pred_original, pred)
pred = sum(betas[i,]*prediction_data_new) + rnorm(1, 0, sqrt(sigmas[i]))
mdv_pred_new = append(mdv_pred_new, pred)
}
hist(mdv_pred_original)
hist(mdv_pred_new)
mean(mdv_pred_original)
mean(mdv_pred_original)
mean(mdv_pred_new)
prediction_data_original
y[,9]
y[9,]
y[9]
difference = mdv_pred_new - mdv_pred_original
difference
plot(difference)
sum(difference>0)/nSim
diffPrice = joint_posterior$betaSample[,14]*(29.93*0.7-29.93)
hist(diffPrice,50)
diffPrice
29.93*0.7-29.93
# which does no contain zero, and the expected price increase is
mean(diffPrice)diffPrice = joint_posterior$betaSample[,14]*(29.93*0.7-29.93)
diffPrice = joint_posterior$betaSample[,14]*(29.93*0.7-29.93)
hist(diffPrice,50)
# 95% HPD for the price difference
quantile(diffPrice,c(0.02,0.975))
# which does no contain zero, and the expected price increase is
mean(diffPrice)
