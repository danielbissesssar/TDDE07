#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
###############################
########## Problem 2 ##########
###############################
###############################
########## Problem 3 ##########
###############################
xData <- c(1.888, 2.954, 0.364, 0.349, 1.090, 7.237)
yData <- c(-1.246, -1.139, -0.358, -1.308, -0.930, -0.157, -0.111, -0.635)
###############################
########## Problem 4 ##########
###############################
# Load airline incidents data
load(file = 'incidents.RData')
# Load airline incidents data
load(file = 'incidents.RData')
setwd("C:/Users/Arun/Bayesian_Learning_TDDE07/Tenta190821")
# Load airline incidents data
load(file = 'incidents.RData')
# Bayesian Learning Exam 2019-08-21
# Run this file once during the exam to get all the required data and functions for the exam in working memory
# Author: Per Siden
###############################
########## Problem 1 ##########
###############################
# Reading the data from file
library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
names(X)[1] <- "intercept"
covNames <- names(X)
y <- as.numeric(y)
X <- as.matrix(X)
XNewHouse <- c(1,0.03,40,1.5,0,0.5,6,30,5,3,300,17,390,4)
if(length((grep("mvtnorm",installed.packages()[,1])))==0)
install.packages("mvtnorm")
library(mvtnorm)
# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
return((df*scale)/rchisq(n,df=df))
}
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
###############################
########## Problem 2 ##########
###############################
###############################
########## Problem 3 ##########
###############################
xData <- c(1.888, 2.954, 0.364, 0.349, 1.090, 7.237)
yData <- c(-1.246, -1.139, -0.358, -1.308, -0.930, -0.157, -0.111, -0.635)
###############################
########## Problem 4 ##########
###############################
# Load airline incidents data
load(file = 'incidents.RData')
my_0 = rep(0, 14)
Omega_0 = diag(0, 14)
Omega_0 = Omega_0*0.01
sigma = 5
v_0 = 1
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = 4, sigma2_0 = sigma^2 , nIter = 5000 )
View(BostonHousing)
X
mean(betas[,6])
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
mean(betas[,6])
sigma_quantile =  quantile(betas[,6], probs = c(00025 0.9725))
sigma_quantile =  quantile(betas[,6], probs = c(0.925 0.9725))
sigma_quantile =  quantile(betas[,6], probs = c(0.925, 0.9725))
quantile(betas[,6], probs = c(0.925, 0.9725))
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = 4, sigma2_0 = sigma^2 , nIter = 5000 )
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
quantile(betas[,6], probs = c(0.925, 0.9725))
sigmaSquare = 5
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = 4, sigma2_0 = sigmaSquare , nIter = 5000 )
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
quantile(betas[,6], probs = c(0.925, 0.9725))
quantile(betas[,6], probs = c(0.025, 0.9725))
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = 4, sigma2_0 = sigmaSquare , nIter = 5000 )
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
quantile(betas[,6], probs = c(0.025, 0.9725))
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = v_0, sigma2_0 = sigmaSquare , nIter = 5000 )
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
quantile(betas[,6], probs = c(0.025, 0.9725))
Bmean = colMeans(joint_posterior$betaSample)
Bmean
Bq025 = apply(joint_posterior$betaSample,2,quantile,.025)
Bq025
X
betas[,6]
sigma_density = density(joint_posterior$sigma2Sample)
plot(sigma_density)
summary(sigma_density)
sigma_density
quantile(sigma_density, probs = c(0.05, 0.095))
yProp
thetaGrid = seq(0.01,15, length =1000)
lambda = 1
posterior.calc = function(y, theta){
#  for(i in length(y)){
#   likelihood = likelihood + (theta-1)*log(y[i])+(theta-1)*log(1-y[i])
# }
# log.post = likelihood + log(lambda*exp(-lambda*theta))
likelihood = sum(log(dgamma(y, theta, theta)))
log.prior = dexp(theta, rate = 1, log = TRUE)
log.post = likelihood + log.prior
return(exp(log.post))
}
posterior = c()
for(theta in thetaGrid){
posterior = append(posterior, posterior.calc(yProp, theta))
}
gridwidth = thetaGrid[2] - thetaGrid[1]
posterior_nromalized = (1/gridwidth)*posterior/sum(posterior)
plot(thetaGrid, posterior_nromalized)
#zero 1 loss => posterior mode
posterior_mode = thetaGrid[which.max(posterior_nromalized)]
## b
log.posterior = function(params, yVect){
theta1 = params[1]
theta2 = params[2]
likelihood = sum(log(dgamma(y, theta1, theta2)))
log.prior1 = dexp(theta1, rate = 1, log = TRUE)
log.prior2 = dexp(theta2, rate = 1, log = TRUE)
log.post = likelihood + log.prior1 + log.prior2
return(log.post)
}
initVal <- as.vector(rep(0,2));
OptParams<-optim(initVal,log.posterior,gr=NULL,yVect,method=c("L-BFGS-B"),lower = c(0.0001,0.0001), control=list(fnscale=-1),hessian=TRUE)
my_posterior = OptParams$par
hessian_posterior = -OptParams$hessian
#2a)
mu_0 = rep(0, ncol(X))
Omega_0 = diag(1, ncol(X))
Omega_0 = Omega_0*0.01
sigma = 6^2
v_0 = 1
nSim = 5000
joint_posterior <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0 = sigma, nIter = nSim)
sort(joint_posterior$betaSample[,14])
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
#ibrary(HDInterval)
hdiRange = hdi(joint_posterior$betaSample[,14], credMass=0.95)
quantile(joint_posterior$betaSample[,14], probs = seq(0,1, 0.025))
hdiRange
#2b
prediction_data_original = X[9,]
prediction_data_new = X[9,]
prediction_data_new[14] =  prediction_data_new[14]*0.7
set.seed(12345)
mdv_pred_original = c()
mdv_pred_new = c()
for (i in 1:nrow(betas)){
pred = sum(betas[i,]*prediction_data_original) + rnorm(1, 0, sqrt(sigmas[i]))
mdv_pred_original = append(mdv_pred_original, pred)
pred = sum(betas[i,]*prediction_data_new) + rnorm(1, 0, sqrt(sigmas[i]))
mdv_pred_new = append(mdv_pred_new, pred)
}
difference = mdv_pred_new - mdv_pred_original
sum(difference>0)/nSim
diffPrice = joint_posterior$betaSample[,14]*(29.93*0.7-29.93)
hist(diffPrice,50)
difference = mdv_pred_new - mdv_pred_original
sum(difference>0)/nSim
hist(difference)
hist(diffPrice,50)
hist(difference, 50)
hist(diffPrice,50)
my_0 = rep(0, 14)
my_0 = rep(0, 14)
Omega_0 = diag(0, 14)
Omega_0 = Omega_0*0.01
sigmaSquare = 5
v_0 = 1
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = v_0, sigma2_0 = sigmaSquare , nIter = 5000 )
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
quantile(betas[,6], probs = c(0.025, 0.9725))
sigma_quantile = c(sigma_quantile[2], sigma_quantile[40])
Bmean = colMeans(joint_posterior$betaSample)
Bq025 = apply(joint_posterior$betaSample,2,quantile,.025)
Bq975 = apply(joint_posterior$betaSample,2,quantile,.975)
print(data.frame(round(cbind(Bmean,Bq025,Bq975),3)),row.names=covNames)
sigma_density = density(joint_posterior$sigma2Sample)
plot(sigma_density)
summary(sigma_density)
sigma_density$
sorted_normalized_y = sort(sigma_density$y, decreasing = TRUE)/sum(sigma_density$y)
sorted_x = sigma_density$x[order(-sigma_density$y)]
count = 0
summa = 0
while(summa <= 0.95){
count = count + 1
summa = sorted_normalized[count] + summa
}
count = 0
summa = 0
while(summa <= 0.95){
count = count + 1
summa = sorted_normalized_y[count] + summa
}
a = min(sorted_x[1:count-1])
b = max(sorted_x[1:count-1])
a
b
mode = sorted_x[which.max(sorted_normalized_y)]
mode
XNewHouse
ypred = XNewHouse*betas + rnorm(5000, 0, sqrt(sigmas))
ypred
joint_posterior = BayesLinReg(y, X, mu_0 = my_0,Omega_0 = Omega_0,  v_0 = v_0, sigma2_0 = sigmaSquare , nIter = 5000 )
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
XNewHouse
ypred = XNewHouse*betas + rnorm(5000, 0, sqrt(sigmas))
ypred
betas
ypred = XNewHouse*betas
ypred
ypred =c()
betas
betas
for(i in 1:5000){
ypred = append(ypred, XNewHouse*betas[i,])
}
ypred
# Bayesian Learning Exam 2019-08-21
# Run this file once during the exam to get all the required data and functions for the exam in working memory
# Author: Per Siden
###############################
########## Problem 1 ##########
###############################
# Reading the data from file
library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
names(X)[1] <- "intercept"
covNames <- names(X)
y <- as.numeric(y)
X <- as.matrix(X)
XNewHouse <- c(1,0.03,40,1.5,0,0.5,6,30,5,3,300,17,390,4)
if(length((grep("mvtnorm",installed.packages()[,1])))==0)
install.packages("mvtnorm")
library(mvtnorm)
# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
return((df*scale)/rchisq(n,df=df))
}
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
###############################
########## Problem 2 ##########
###############################
###############################
########## Problem 3 ##########
###############################
xData <- c(1.888, 2.954, 0.364, 0.349, 1.090, 7.237)
yData <- c(-1.246, -1.139, -0.358, -1.308, -0.930, -0.157, -0.111, -0.635)
###############################
########## Problem 4 ##########
###############################
# Load airline incidents data
load(file = 'incidents.RData')
nCovs = dim(X)[2]
mu_0 = rep(0,nCovs)
Omega_0 = (1/100)*diag(nCovs)
v_0 = 1
sigma2_0 = 5^2
joint_posterior = BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter = 5000)
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
nSim <- dim(BostonRes$betaSample)[1]
nSim <- dim(joint_posterior$betaSample)[1]
nSim
ypred =c()
for(i in 1:5000){
ypred = append(ypred, XNewHouse*betas[i,])
}
ypred
XNewHouse
for(i in 1:5000){
ypred = append(ypred, XNewHouse%*%betas[i,])
}
ypred =c()
for(i in 1:5000){
ypred = append(ypred, XNewHouse%*%betas[i,])
}
ypred
ypred = XNewHouse%*%betas
joint_posterior = BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter = 5000)
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
ypred = XNewHouse%*%betas
ypred = XNewHouse*betas
ypred
View(betas)
for(i in 1:5000){
ypred = append(ypred, XNewHouse%*%betas[i,]) + rnorm(1, 0, sqrt(sigmas[i]))
}
ypred
for(i in 1:5000){
ypred = append(ypred, XNewHouse%*%betas[i,]) )
}
for(i in 1:5000){
ypred = append(ypred, XNewHouse%*%betas[i,])
}
ypred
ypred = XNewHouse*betas
for(i in 1:5000){
ypred = append(ypred, XNewHouse%*%betas[i,]) + rnorm(1, 0, sqrt(sigmas[i]))
}
Y
y
BostonHousing
sum(ypred>20)/nSim
for (i in 1:nSim){
yPred[i] <- XNewHouse%*%BostonRes$betaSample[i,] + rnorm(n = 1, mean = 0, sd = sqrt(BostonRes$sigma2Sample[i]))
}
nCovs = dim(X)[2]
mu_0 = rep(0,nCovs)
Omega_0 = (1/100)*diag(nCovs)
v_0 = 1
sigma2_0 = 5^2
BostonRes <-  BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter = 5000)
Bmean = colMeans(BostonRes$betaSample)
Bq025 = apply(BostonRes$betaSample,2,quantile,.025)
Bq975 = apply(BostonRes$betaSample,2,quantile,.975)
print(data.frame(round(cbind(Bmean,Bq025,Bq975),3)),row.names=covNames)
With 95% probability, a higher nox by 1 part per 10 million is associated with a house price that is between 10 and 25 thousand dollars lower.
### 1b
```{r}
sigma2Sample = BostonRes$sigma2Sample
dS = density(sigma2Sample)
par(mfrow=c(1,1))
plot(dS,main="Kernel density posterior for sigma2")
dn=sort(dS$y/sum(dS$y),index.return=TRUE);
dnn = cumsum(dn$x)
HPDind = sort(dn$ix[dnn > .1])
HPDCI = c(min(dS$x[HPDind]),max(dS$x[HPDind]))
cat("Posterior mode: ",dS$x[which.max(dS$y)])
cat("Highest posterior density interval: ",HPDCI)
nSim <- dim(BostonRes$betaSample)[1] # One predictive draw for each posterior parameter draw
yPred <- matrix(0,nSim,1)
for (i in 1:nSim){
yPred[i] <- XNewHouse%*%BostonRes$betaSample[i,] + rnorm(n = 1, mean = 0, sd = sqrt(BostonRes$sigma2Sample[i]))
}
par(mfrow=c(1,1))
hist(yPred,50)
sum(yPred>=20)/nSim
ypred c()
nCovs = dim(X)[2]
mu_0 = rep(0,nCovs)
Omega_0 = (1/100)*diag(nCovs)
v_0 = 1
sigma2_0 = 5^2
joint_posterior = BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter = 5000)
betas = joint_posterior$betaSample
sigmas = joint_posterior$sigma2Sample
quantile(betas[,6], probs = c(0.025, 0.9725))
sigma_quantile = c(sigma_quantile[2], sigma_quantile[40])
Bmean = colMeans(joint_posterior$betaSample)
Bq025 = apply(joint_posterior$betaSample,2,quantile,.025)
Bq975 = apply(joint_posterior$betaSample,2,quantile,.975)
print(data.frame(round(cbind(Bmean,Bq025,Bq975),3)),row.names=covNames)
#B compute density
sigma_density = density(joint_posterior$sigma2Sample)
plot(sigma_density)
summary(sigma_density)
sorted_normalized_y = sort(sigma_density$y, decreasing = TRUE)/sum(sigma_density$y)
sorted_x = sigma_density$x[order(-sigma_density$y)]
count = 0
summa = 0
while(summa <= 0.95){
count = count + 1
summa = sorted_normalized_y[count] + summa
}
a = min(sorted_x[1:count-1])
b = max(sorted_x[1:count-1])
mode = sorted_x[which.max(sorted_normalized_y)]
XNewHouse
nSim <- dim(joint_posterior$betaSample)[1]
ypred = c()
for(i in 1:5000){
ypred = append(ypred, XNewHouse%*%betas[i,]) + rnorm(1, 0, sqrt(sigmas[i]))
}
sum(ypred>20)/nSim
